{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train = MNIST(\n",
    "    root=\"./dara/\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform\n",
    ")\n",
    "val = MNIST(\n",
    "    root=\"./data/\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\n",
    "        loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\n",
    "        return loader\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'val_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # TODO: do a real train/val split\n",
    "        dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
    "        loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "        return loader\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # TODO: do a real train/val split\n",
    "        dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
    "        loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "        return loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type   | Params\n",
      "----------------------------\n",
      "0 | l1   | Linear | 7 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 1875/2188 [00:11<00:01, 157.57it/s, loss=0.989, v_num=11]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 1879/2188 [00:12<00:01, 156.26it/s, loss=0.989, v_num=11]\n",
      "Epoch 1:  88%|████████▊ | 1919/2188 [00:12<00:01, 158.26it/s, loss=0.989, v_num=11]\n",
      "Epoch 1:  91%|█████████ | 1993/2188 [00:12<00:01, 163.00it/s, loss=0.989, v_num=11]\n",
      "Epoch 1:  95%|█████████▍| 2074/2188 [00:12<00:00, 168.24it/s, loss=0.989, v_num=11]\n",
      "Epoch 1: 100%|██████████| 2188/2188 [00:12<00:00, 174.86it/s, loss=0.989, v_num=11]\n",
      "Epoch 2:   0%|          | 0/2188 [00:00<?, ?it/s, loss=0.989, v_num=11]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Did not find hyperparameters at model hparams. Saving checkpoint without hyperparameters.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 1875/2188 [00:11<00:01, 164.92it/s, loss=0.945, v_num=11]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|▏         | 4/313 [00:00<00:08, 35.04it/s]\u001b[A\n",
      "Epoch 2:  89%|████████▉ | 1944/2188 [00:11<00:01, 167.49it/s, loss=0.945, v_num=11]\n",
      "Epoch 2:  93%|█████████▎| 2025/2188 [00:11<00:00, 172.95it/s, loss=0.945, v_num=11]\n",
      "Epoch 2:  96%|█████████▋| 2106/2188 [00:11<00:00, 178.28it/s, loss=0.945, v_num=11]\n",
      "Epoch 2: 100%|██████████| 2188/2188 [00:11<00:00, 183.28it/s, loss=0.945, v_num=11]\n",
      "Epoch 3:  86%|████████▌ | 1875/2188 [00:11<00:01, 167.44it/s, loss=0.990, v_num=11]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|▏         | 4/313 [00:00<00:08, 36.53it/s]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 1944/2188 [00:11<00:01, 170.15it/s, loss=0.990, v_num=11]\n",
      "Epoch 3:  93%|█████████▎| 2025/2188 [00:11<00:00, 175.66it/s, loss=0.990, v_num=11]\n",
      "Epoch 3:  96%|█████████▋| 2108/2188 [00:11<00:00, 181.28it/s, loss=0.990, v_num=11]\n",
      "Epoch 3: 100%|██████████| 2188/2188 [00:11<00:00, 185.83it/s, loss=0.990, v_num=11]\n",
      "Epoch 4:  86%|████████▌ | 1875/2188 [00:11<00:01, 162.24it/s, loss=0.995, v_num=11]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4:  87%|████████▋ | 1909/2188 [00:11<00:01, 162.94it/s, loss=0.995, v_num=11]\n",
      "Epoch 4:  91%|█████████ | 1992/2188 [00:11<00:01, 168.55it/s, loss=0.995, v_num=11]\n",
      "Epoch 4:  95%|█████████▍| 2076/2188 [00:11<00:00, 174.18it/s, loss=0.995, v_num=11]\n",
      "Epoch 4: 100%|██████████| 2188/2188 [00:12<00:00, 180.97it/s, loss=0.995, v_num=11]\n",
      "Epoch 5:  86%|████████▌ | 1875/2188 [00:11<00:01, 170.42it/s, loss=0.938, v_num=11]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  88%|████████▊ | 1932/2188 [00:11<00:01, 172.30it/s, loss=0.938, v_num=11]\n",
      "Epoch 5:  92%|█████████▏| 2016/2188 [00:11<00:00, 178.15it/s, loss=0.938, v_num=11]\n",
      "Epoch 5:  96%|█████████▌| 2100/2188 [00:11<00:00, 183.91it/s, loss=0.938, v_num=11]\n",
      "Epoch 5: 100%|██████████| 2188/2188 [00:11<00:00, 189.23it/s, loss=0.938, v_num=11]\n",
      "Epoch 6:  72%|███████▏  | 1567/2188 [00:09<00:03, 167.47it/s, loss=0.973, v_num=11]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  79%|███████▊  | 246/313 [00:00<00:02, 25.08it/s]--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_loss': tensor(1.1330, device='cuda:0'),\n",
      " 'test_loss': tensor(1.1330, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 313/313 [00:00<00:00, 602.65it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fb0a448cb50>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
