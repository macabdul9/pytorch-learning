{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear-regression-using-built-ins.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVlzraTsxF7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcNfxGmCxT3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpt6j877xWqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = torch.tensor([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
        "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
        "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
        "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
        "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
        "                  dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets = torch.tensor([[56, 70], [81, 101], [119, 133], \n",
        "                    [22, 37], [103, 119], [56, 70], \n",
        "                    [81, 101], [119, 133], [22, 37], \n",
        "                    [103, 119], [56, 70], [81, 101], \n",
        "                    [119, 133], [22, 37], [103, 119]], \n",
        "                   dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CakLr49txrTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4830f7e5-75c4-4d1f-cb0b-ac9c451d8f65"
      },
      "source": [
        "print(inputs.shape, targets.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15, 3]) torch.Size([15, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDqsK3QsyhJu",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Data Loader\n",
        "- TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch.\n",
        "- DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CPbpxkdxwVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9tsxWtAyyQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = TensorDataset(inputs, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJaaQ0RwzM6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = DataLoader(dataset, 8, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGs1i9oJziRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b0ed9c3-759a-4a2c-d37d-406cbc033716"
      },
      "source": [
        "for x, y in train_gen:\n",
        "  print(x.shape, y.shape)\n",
        "  break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3]) torch.Size([8, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbAdjrrdzmbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Linear(3, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAvoMj0Y98Q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "e681fcce-c7c9-4bdb-9caf-15ce921cda66"
      },
      "source": [
        "model(inputs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 54.0886,  61.8167],\n",
              "        [ 64.8209,  84.6916],\n",
              "        [ 90.3813, 105.4837],\n",
              "        [ 59.1094,  48.3616],\n",
              "        [ 54.8224,  90.2787],\n",
              "        [ 54.0886,  61.8167],\n",
              "        [ 64.8209,  84.6916],\n",
              "        [ 90.3813, 105.4837],\n",
              "        [ 59.1094,  48.3616],\n",
              "        [ 54.8224,  90.2787],\n",
              "        [ 54.0886,  61.8167],\n",
              "        [ 64.8209,  84.6916],\n",
              "        [ 90.3813, 105.4837],\n",
              "        [ 59.1094,  48.3616],\n",
              "        [ 54.8224,  90.2787]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OPeGyV-Cfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "fd9a6d9a-819d-429f-f10e-eca503d621d8"
      },
      "source": [
        "targets"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxzd9vg--WXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqXxbEmu_mHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = F.mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOLo7I2k_rL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2212a8f-9fc2-49c8-9849-91f156d7b884"
      },
      "source": [
        "loss_fn(model(inputs), targets)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(682.6704, grad_fn=<MseLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptAXf8nU_zJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = torch.optim.SGD(model.parameters(), 1e-05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1SBeTTwAGaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "    \n",
        "    # Repeat for given number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # Train with batches of data\n",
        "        for xb,yb in train_gen:\n",
        "            \n",
        "            # 1. Generate predictions\n",
        "            pred = model(xb)\n",
        "            \n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(pred, yb)\n",
        "            \n",
        "            # 3. Compute gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # 4. Update parameters using gradients\n",
        "            opt.step()\n",
        "            \n",
        "            # 5. Reset the gradients to zero\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        # Print the progress\n",
        "        if (epoch+1) % 1000 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-vQrM6uA2a9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7eacbf21-b61f-4b84-9a8f-bf1f533bc37a"
      },
      "source": [
        "fit(10000, model, loss_fn, opt)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/10000], Loss: 1.4982\n",
            "Epoch [20/10000], Loss: 1.9026\n",
            "Epoch [30/10000], Loss: 0.9709\n",
            "Epoch [40/10000], Loss: 0.8611\n",
            "Epoch [50/10000], Loss: 1.0027\n",
            "Epoch [60/10000], Loss: 1.3307\n",
            "Epoch [70/10000], Loss: 1.7748\n",
            "Epoch [80/10000], Loss: 1.1337\n",
            "Epoch [90/10000], Loss: 0.6356\n",
            "Epoch [100/10000], Loss: 1.2199\n",
            "Epoch [110/10000], Loss: 1.2477\n",
            "Epoch [120/10000], Loss: 1.3152\n",
            "Epoch [130/10000], Loss: 0.8787\n",
            "Epoch [140/10000], Loss: 1.4913\n",
            "Epoch [150/10000], Loss: 1.0629\n",
            "Epoch [160/10000], Loss: 0.6861\n",
            "Epoch [170/10000], Loss: 0.7750\n",
            "Epoch [180/10000], Loss: 0.9367\n",
            "Epoch [190/10000], Loss: 1.1683\n",
            "Epoch [200/10000], Loss: 0.9998\n",
            "Epoch [210/10000], Loss: 0.7897\n",
            "Epoch [220/10000], Loss: 0.5330\n",
            "Epoch [230/10000], Loss: 0.9496\n",
            "Epoch [240/10000], Loss: 0.7395\n",
            "Epoch [250/10000], Loss: 0.6289\n",
            "Epoch [260/10000], Loss: 0.9843\n",
            "Epoch [270/10000], Loss: 0.6358\n",
            "Epoch [280/10000], Loss: 0.5885\n",
            "Epoch [290/10000], Loss: 0.6149\n",
            "Epoch [300/10000], Loss: 1.2399\n",
            "Epoch [310/10000], Loss: 0.3887\n",
            "Epoch [320/10000], Loss: 0.5760\n",
            "Epoch [330/10000], Loss: 0.6645\n",
            "Epoch [340/10000], Loss: 0.7606\n",
            "Epoch [350/10000], Loss: 0.3932\n",
            "Epoch [360/10000], Loss: 0.7167\n",
            "Epoch [370/10000], Loss: 0.6144\n",
            "Epoch [380/10000], Loss: 0.4769\n",
            "Epoch [390/10000], Loss: 0.6259\n",
            "Epoch [400/10000], Loss: 0.5847\n",
            "Epoch [410/10000], Loss: 0.5936\n",
            "Epoch [420/10000], Loss: 0.4769\n",
            "Epoch [430/10000], Loss: 0.6007\n",
            "Epoch [440/10000], Loss: 0.9117\n",
            "Epoch [450/10000], Loss: 0.5438\n",
            "Epoch [460/10000], Loss: 0.6918\n",
            "Epoch [470/10000], Loss: 0.5535\n",
            "Epoch [480/10000], Loss: 0.6065\n",
            "Epoch [490/10000], Loss: 0.4401\n",
            "Epoch [500/10000], Loss: 0.5621\n",
            "Epoch [510/10000], Loss: 0.9015\n",
            "Epoch [520/10000], Loss: 0.9310\n",
            "Epoch [530/10000], Loss: 0.4298\n",
            "Epoch [540/10000], Loss: 0.5885\n",
            "Epoch [550/10000], Loss: 0.3642\n",
            "Epoch [560/10000], Loss: 0.7952\n",
            "Epoch [570/10000], Loss: 0.4738\n",
            "Epoch [580/10000], Loss: 0.6825\n",
            "Epoch [590/10000], Loss: 0.4556\n",
            "Epoch [600/10000], Loss: 0.4793\n",
            "Epoch [610/10000], Loss: 0.5194\n",
            "Epoch [620/10000], Loss: 0.4965\n",
            "Epoch [630/10000], Loss: 0.4607\n",
            "Epoch [640/10000], Loss: 0.3437\n",
            "Epoch [650/10000], Loss: 0.6095\n",
            "Epoch [660/10000], Loss: 0.7418\n",
            "Epoch [670/10000], Loss: 0.4114\n",
            "Epoch [680/10000], Loss: 0.6370\n",
            "Epoch [690/10000], Loss: 0.7036\n",
            "Epoch [700/10000], Loss: 0.3809\n",
            "Epoch [710/10000], Loss: 0.5401\n",
            "Epoch [720/10000], Loss: 0.5626\n",
            "Epoch [730/10000], Loss: 0.7450\n",
            "Epoch [740/10000], Loss: 0.5549\n",
            "Epoch [750/10000], Loss: 0.4938\n",
            "Epoch [760/10000], Loss: 0.4789\n",
            "Epoch [770/10000], Loss: 0.7406\n",
            "Epoch [780/10000], Loss: 0.6066\n",
            "Epoch [790/10000], Loss: 0.4972\n",
            "Epoch [800/10000], Loss: 0.7438\n",
            "Epoch [810/10000], Loss: 0.6295\n",
            "Epoch [820/10000], Loss: 0.5229\n",
            "Epoch [830/10000], Loss: 0.6650\n",
            "Epoch [840/10000], Loss: 0.5605\n",
            "Epoch [850/10000], Loss: 0.4004\n",
            "Epoch [860/10000], Loss: 0.5370\n",
            "Epoch [870/10000], Loss: 0.4382\n",
            "Epoch [880/10000], Loss: 0.5968\n",
            "Epoch [890/10000], Loss: 0.5044\n",
            "Epoch [900/10000], Loss: 0.4377\n",
            "Epoch [910/10000], Loss: 0.6446\n",
            "Epoch [920/10000], Loss: 0.5849\n",
            "Epoch [930/10000], Loss: 0.4810\n",
            "Epoch [940/10000], Loss: 0.4953\n",
            "Epoch [950/10000], Loss: 0.5566\n",
            "Epoch [960/10000], Loss: 0.4446\n",
            "Epoch [970/10000], Loss: 0.5933\n",
            "Epoch [980/10000], Loss: 0.6753\n",
            "Epoch [990/10000], Loss: 0.5096\n",
            "Epoch [1000/10000], Loss: 0.5918\n",
            "Epoch [1010/10000], Loss: 0.4496\n",
            "Epoch [1020/10000], Loss: 0.6038\n",
            "Epoch [1030/10000], Loss: 0.5448\n",
            "Epoch [1040/10000], Loss: 0.4724\n",
            "Epoch [1050/10000], Loss: 0.5455\n",
            "Epoch [1060/10000], Loss: 0.4770\n",
            "Epoch [1070/10000], Loss: 0.4722\n",
            "Epoch [1080/10000], Loss: 0.4628\n",
            "Epoch [1090/10000], Loss: 0.5840\n",
            "Epoch [1100/10000], Loss: 0.6454\n",
            "Epoch [1110/10000], Loss: 0.5605\n",
            "Epoch [1120/10000], Loss: 0.6150\n",
            "Epoch [1130/10000], Loss: 0.5179\n",
            "Epoch [1140/10000], Loss: 0.6038\n",
            "Epoch [1150/10000], Loss: 0.4288\n",
            "Epoch [1160/10000], Loss: 0.6348\n",
            "Epoch [1170/10000], Loss: 0.5219\n",
            "Epoch [1180/10000], Loss: 0.6384\n",
            "Epoch [1190/10000], Loss: 0.6586\n",
            "Epoch [1200/10000], Loss: 0.5882\n",
            "Epoch [1210/10000], Loss: 0.4968\n",
            "Epoch [1220/10000], Loss: 0.5424\n",
            "Epoch [1230/10000], Loss: 0.4894\n",
            "Epoch [1240/10000], Loss: 0.5227\n",
            "Epoch [1250/10000], Loss: 0.4758\n",
            "Epoch [1260/10000], Loss: 0.5470\n",
            "Epoch [1270/10000], Loss: 0.6805\n",
            "Epoch [1280/10000], Loss: 0.5237\n",
            "Epoch [1290/10000], Loss: 0.5922\n",
            "Epoch [1300/10000], Loss: 0.4769\n",
            "Epoch [1310/10000], Loss: 0.4870\n",
            "Epoch [1320/10000], Loss: 0.6091\n",
            "Epoch [1330/10000], Loss: 0.6777\n",
            "Epoch [1340/10000], Loss: 0.5280\n",
            "Epoch [1350/10000], Loss: 0.3761\n",
            "Epoch [1360/10000], Loss: 0.2843\n",
            "Epoch [1370/10000], Loss: 0.6770\n",
            "Epoch [1380/10000], Loss: 0.5906\n",
            "Epoch [1390/10000], Loss: 0.5866\n",
            "Epoch [1400/10000], Loss: 0.6620\n",
            "Epoch [1410/10000], Loss: 0.5536\n",
            "Epoch [1420/10000], Loss: 0.5975\n",
            "Epoch [1430/10000], Loss: 0.4394\n",
            "Epoch [1440/10000], Loss: 0.5310\n",
            "Epoch [1450/10000], Loss: 0.5402\n",
            "Epoch [1460/10000], Loss: 0.5403\n",
            "Epoch [1470/10000], Loss: 0.3349\n",
            "Epoch [1480/10000], Loss: 0.5406\n",
            "Epoch [1490/10000], Loss: 0.3412\n",
            "Epoch [1500/10000], Loss: 0.7020\n",
            "Epoch [1510/10000], Loss: 0.4465\n",
            "Epoch [1520/10000], Loss: 0.6083\n",
            "Epoch [1530/10000], Loss: 0.6333\n",
            "Epoch [1540/10000], Loss: 0.5268\n",
            "Epoch [1550/10000], Loss: 0.4784\n",
            "Epoch [1560/10000], Loss: 0.6331\n",
            "Epoch [1570/10000], Loss: 0.5876\n",
            "Epoch [1580/10000], Loss: 0.4661\n",
            "Epoch [1590/10000], Loss: 0.5962\n",
            "Epoch [1600/10000], Loss: 0.5071\n",
            "Epoch [1610/10000], Loss: 0.4461\n",
            "Epoch [1620/10000], Loss: 0.6186\n",
            "Epoch [1630/10000], Loss: 0.5287\n",
            "Epoch [1640/10000], Loss: 0.6274\n",
            "Epoch [1650/10000], Loss: 0.6303\n",
            "Epoch [1660/10000], Loss: 0.5977\n",
            "Epoch [1670/10000], Loss: 0.5993\n",
            "Epoch [1680/10000], Loss: 0.5233\n",
            "Epoch [1690/10000], Loss: 0.4940\n",
            "Epoch [1700/10000], Loss: 0.5618\n",
            "Epoch [1710/10000], Loss: 0.5538\n",
            "Epoch [1720/10000], Loss: 0.5615\n",
            "Epoch [1730/10000], Loss: 0.5616\n",
            "Epoch [1740/10000], Loss: 0.5934\n",
            "Epoch [1750/10000], Loss: 0.5289\n",
            "Epoch [1760/10000], Loss: 0.3345\n",
            "Epoch [1770/10000], Loss: 0.4783\n",
            "Epoch [1780/10000], Loss: 0.4725\n",
            "Epoch [1790/10000], Loss: 0.5898\n",
            "Epoch [1800/10000], Loss: 0.4796\n",
            "Epoch [1810/10000], Loss: 0.4772\n",
            "Epoch [1820/10000], Loss: 0.4021\n",
            "Epoch [1830/10000], Loss: 0.4723\n",
            "Epoch [1840/10000], Loss: 0.5304\n",
            "Epoch [1850/10000], Loss: 0.4312\n",
            "Epoch [1860/10000], Loss: 0.4624\n",
            "Epoch [1870/10000], Loss: 0.4389\n",
            "Epoch [1880/10000], Loss: 0.4741\n",
            "Epoch [1890/10000], Loss: 0.4575\n",
            "Epoch [1900/10000], Loss: 0.4673\n",
            "Epoch [1910/10000], Loss: 0.4713\n",
            "Epoch [1920/10000], Loss: 0.5620\n",
            "Epoch [1930/10000], Loss: 0.4891\n",
            "Epoch [1940/10000], Loss: 0.6029\n",
            "Epoch [1950/10000], Loss: 0.6270\n",
            "Epoch [1960/10000], Loss: 0.4603\n",
            "Epoch [1970/10000], Loss: 0.6240\n",
            "Epoch [1980/10000], Loss: 0.5594\n",
            "Epoch [1990/10000], Loss: 0.4572\n",
            "Epoch [2000/10000], Loss: 0.5481\n",
            "Epoch [2010/10000], Loss: 0.6235\n",
            "Epoch [2020/10000], Loss: 0.4727\n",
            "Epoch [2030/10000], Loss: 0.6284\n",
            "Epoch [2040/10000], Loss: 0.6865\n",
            "Epoch [2050/10000], Loss: 0.4491\n",
            "Epoch [2060/10000], Loss: 0.6221\n",
            "Epoch [2070/10000], Loss: 0.4769\n",
            "Epoch [2080/10000], Loss: 0.4591\n",
            "Epoch [2090/10000], Loss: 0.4790\n",
            "Epoch [2100/10000], Loss: 0.5600\n",
            "Epoch [2110/10000], Loss: 0.4782\n",
            "Epoch [2120/10000], Loss: 0.5931\n",
            "Epoch [2130/10000], Loss: 0.5357\n",
            "Epoch [2140/10000], Loss: 0.3648\n",
            "Epoch [2150/10000], Loss: 0.5580\n",
            "Epoch [2160/10000], Loss: 0.6314\n",
            "Epoch [2170/10000], Loss: 0.5405\n",
            "Epoch [2180/10000], Loss: 0.6690\n",
            "Epoch [2190/10000], Loss: 0.5924\n",
            "Epoch [2200/10000], Loss: 0.4155\n",
            "Epoch [2210/10000], Loss: 0.3541\n",
            "Epoch [2220/10000], Loss: 0.5948\n",
            "Epoch [2230/10000], Loss: 0.4838\n",
            "Epoch [2240/10000], Loss: 0.5977\n",
            "Epoch [2250/10000], Loss: 0.5523\n",
            "Epoch [2260/10000], Loss: 0.4701\n",
            "Epoch [2270/10000], Loss: 0.4877\n",
            "Epoch [2280/10000], Loss: 0.3595\n",
            "Epoch [2290/10000], Loss: 0.3814\n",
            "Epoch [2300/10000], Loss: 0.5410\n",
            "Epoch [2310/10000], Loss: 0.5955\n",
            "Epoch [2320/10000], Loss: 0.5308\n",
            "Epoch [2330/10000], Loss: 0.6115\n",
            "Epoch [2340/10000], Loss: 0.4872\n",
            "Epoch [2350/10000], Loss: 0.5318\n",
            "Epoch [2360/10000], Loss: 0.4672\n",
            "Epoch [2370/10000], Loss: 0.5410\n",
            "Epoch [2380/10000], Loss: 0.4496\n",
            "Epoch [2390/10000], Loss: 0.5353\n",
            "Epoch [2400/10000], Loss: 0.4672\n",
            "Epoch [2410/10000], Loss: 0.5567\n",
            "Epoch [2420/10000], Loss: 0.5968\n",
            "Epoch [2430/10000], Loss: 0.5409\n",
            "Epoch [2440/10000], Loss: 0.5060\n",
            "Epoch [2450/10000], Loss: 0.5508\n",
            "Epoch [2460/10000], Loss: 0.6278\n",
            "Epoch [2470/10000], Loss: 0.5409\n",
            "Epoch [2480/10000], Loss: 0.4677\n",
            "Epoch [2490/10000], Loss: 0.4989\n",
            "Epoch [2500/10000], Loss: 0.5568\n",
            "Epoch [2510/10000], Loss: 0.5654\n",
            "Epoch [2520/10000], Loss: 0.4875\n",
            "Epoch [2530/10000], Loss: 0.5396\n",
            "Epoch [2540/10000], Loss: 0.3546\n",
            "Epoch [2550/10000], Loss: 0.5569\n",
            "Epoch [2560/10000], Loss: 0.6685\n",
            "Epoch [2570/10000], Loss: 0.5189\n",
            "Epoch [2580/10000], Loss: 0.3759\n",
            "Epoch [2590/10000], Loss: 0.6578\n",
            "Epoch [2600/10000], Loss: 0.5656\n",
            "Epoch [2610/10000], Loss: 0.6109\n",
            "Epoch [2620/10000], Loss: 0.5031\n",
            "Epoch [2630/10000], Loss: 0.5559\n",
            "Epoch [2640/10000], Loss: 0.3744\n",
            "Epoch [2650/10000], Loss: 0.5500\n",
            "Epoch [2660/10000], Loss: 0.4915\n",
            "Epoch [2670/10000], Loss: 0.4589\n",
            "Epoch [2680/10000], Loss: 0.5567\n",
            "Epoch [2690/10000], Loss: 0.3975\n",
            "Epoch [2700/10000], Loss: 0.5649\n",
            "Epoch [2710/10000], Loss: 0.5370\n",
            "Epoch [2720/10000], Loss: 0.5665\n",
            "Epoch [2730/10000], Loss: 0.5575\n",
            "Epoch [2740/10000], Loss: 0.6205\n",
            "Epoch [2750/10000], Loss: 0.5319\n",
            "Epoch [2760/10000], Loss: 0.7620\n",
            "Epoch [2770/10000], Loss: 0.4686\n",
            "Epoch [2780/10000], Loss: 0.6666\n",
            "Epoch [2790/10000], Loss: 0.4898\n",
            "Epoch [2800/10000], Loss: 0.6718\n",
            "Epoch [2810/10000], Loss: 0.5104\n",
            "Epoch [2820/10000], Loss: 0.4939\n",
            "Epoch [2830/10000], Loss: 0.5472\n",
            "Epoch [2840/10000], Loss: 0.5652\n",
            "Epoch [2850/10000], Loss: 0.5959\n",
            "Epoch [2860/10000], Loss: 0.5095\n",
            "Epoch [2870/10000], Loss: 0.5335\n",
            "Epoch [2880/10000], Loss: 0.5234\n",
            "Epoch [2890/10000], Loss: 0.3653\n",
            "Epoch [2900/10000], Loss: 0.4466\n",
            "Epoch [2910/10000], Loss: 0.4154\n",
            "Epoch [2920/10000], Loss: 0.6784\n",
            "Epoch [2930/10000], Loss: 0.5506\n",
            "Epoch [2940/10000], Loss: 0.5408\n",
            "Epoch [2950/10000], Loss: 0.4799\n",
            "Epoch [2960/10000], Loss: 0.5407\n",
            "Epoch [2970/10000], Loss: 0.4376\n",
            "Epoch [2980/10000], Loss: 0.5811\n",
            "Epoch [2990/10000], Loss: 0.5339\n",
            "Epoch [3000/10000], Loss: 0.5088\n",
            "Epoch [3010/10000], Loss: 0.4056\n",
            "Epoch [3020/10000], Loss: 0.4384\n",
            "Epoch [3030/10000], Loss: 0.4628\n",
            "Epoch [3040/10000], Loss: 0.5224\n",
            "Epoch [3050/10000], Loss: 0.5410\n",
            "Epoch [3060/10000], Loss: 0.6025\n",
            "Epoch [3070/10000], Loss: 0.4451\n",
            "Epoch [3080/10000], Loss: 0.6077\n",
            "Epoch [3090/10000], Loss: 0.5967\n",
            "Epoch [3100/10000], Loss: 0.3606\n",
            "Epoch [3110/10000], Loss: 0.5977\n",
            "Epoch [3120/10000], Loss: 0.4660\n",
            "Epoch [3130/10000], Loss: 0.3616\n",
            "Epoch [3140/10000], Loss: 0.4879\n",
            "Epoch [3150/10000], Loss: 0.4672\n",
            "Epoch [3160/10000], Loss: 0.6105\n",
            "Epoch [3170/10000], Loss: 0.3595\n",
            "Epoch [3180/10000], Loss: 0.5339\n",
            "Epoch [3190/10000], Loss: 0.5534\n",
            "Epoch [3200/10000], Loss: 0.5326\n",
            "Epoch [3210/10000], Loss: 0.5557\n",
            "Epoch [3220/10000], Loss: 0.5566\n",
            "Epoch [3230/10000], Loss: 0.7177\n",
            "Epoch [3240/10000], Loss: 0.5857\n",
            "Epoch [3250/10000], Loss: 0.5562\n",
            "Epoch [3260/10000], Loss: 0.5639\n",
            "Epoch [3270/10000], Loss: 0.6698\n",
            "Epoch [3280/10000], Loss: 0.5361\n",
            "Epoch [3290/10000], Loss: 0.6740\n",
            "Epoch [3300/10000], Loss: 0.5406\n",
            "Epoch [3310/10000], Loss: 0.6815\n",
            "Epoch [3320/10000], Loss: 0.5036\n",
            "Epoch [3330/10000], Loss: 0.4800\n",
            "Epoch [3340/10000], Loss: 0.3577\n",
            "Epoch [3350/10000], Loss: 0.5147\n",
            "Epoch [3360/10000], Loss: 0.5411\n",
            "Epoch [3370/10000], Loss: 0.5631\n",
            "Epoch [3380/10000], Loss: 0.5128\n",
            "Epoch [3390/10000], Loss: 0.6909\n",
            "Epoch [3400/10000], Loss: 0.4382\n",
            "Epoch [3410/10000], Loss: 0.5090\n",
            "Epoch [3420/10000], Loss: 0.5637\n",
            "Epoch [3430/10000], Loss: 0.4878\n",
            "Epoch [3440/10000], Loss: 0.4306\n",
            "Epoch [3450/10000], Loss: 0.6186\n",
            "Epoch [3460/10000], Loss: 0.6509\n",
            "Epoch [3470/10000], Loss: 0.4369\n",
            "Epoch [3480/10000], Loss: 0.5557\n",
            "Epoch [3490/10000], Loss: 0.5994\n",
            "Epoch [3500/10000], Loss: 0.6604\n",
            "Epoch [3510/10000], Loss: 0.5487\n",
            "Epoch [3520/10000], Loss: 0.4370\n",
            "Epoch [3530/10000], Loss: 0.5413\n",
            "Epoch [3540/10000], Loss: 0.4514\n",
            "Epoch [3550/10000], Loss: 0.5091\n",
            "Epoch [3560/10000], Loss: 0.5208\n",
            "Epoch [3570/10000], Loss: 0.5417\n",
            "Epoch [3580/10000], Loss: 0.5390\n",
            "Epoch [3590/10000], Loss: 0.5475\n",
            "Epoch [3600/10000], Loss: 0.5351\n",
            "Epoch [3610/10000], Loss: 0.5565\n",
            "Epoch [3620/10000], Loss: 0.4840\n",
            "Epoch [3630/10000], Loss: 0.4736\n",
            "Epoch [3640/10000], Loss: 0.5808\n",
            "Epoch [3650/10000], Loss: 0.6159\n",
            "Epoch [3660/10000], Loss: 0.4434\n",
            "Epoch [3670/10000], Loss: 0.4882\n",
            "Epoch [3680/10000], Loss: 0.5104\n",
            "Epoch [3690/10000], Loss: 0.5634\n",
            "Epoch [3700/10000], Loss: 0.5992\n",
            "Epoch [3710/10000], Loss: 0.6001\n",
            "Epoch [3720/10000], Loss: 0.4958\n",
            "Epoch [3730/10000], Loss: 0.4833\n",
            "Epoch [3740/10000], Loss: 0.5420\n",
            "Epoch [3750/10000], Loss: 0.4706\n",
            "Epoch [3760/10000], Loss: 0.4541\n",
            "Epoch [3770/10000], Loss: 0.4307\n",
            "Epoch [3780/10000], Loss: 0.4691\n",
            "Epoch [3790/10000], Loss: 0.6039\n",
            "Epoch [3800/10000], Loss: 0.4872\n",
            "Epoch [3810/10000], Loss: 0.6029\n",
            "Epoch [3820/10000], Loss: 0.5115\n",
            "Epoch [3830/10000], Loss: 0.5488\n",
            "Epoch [3840/10000], Loss: 0.6771\n",
            "Epoch [3850/10000], Loss: 0.4180\n",
            "Epoch [3860/10000], Loss: 0.4924\n",
            "Epoch [3870/10000], Loss: 0.5415\n",
            "Epoch [3880/10000], Loss: 0.4199\n",
            "Epoch [3890/10000], Loss: 0.4328\n",
            "Epoch [3900/10000], Loss: 0.5257\n",
            "Epoch [3910/10000], Loss: 0.4194\n",
            "Epoch [3920/10000], Loss: 0.5452\n",
            "Epoch [3930/10000], Loss: 0.5405\n",
            "Epoch [3940/10000], Loss: 0.3659\n",
            "Epoch [3950/10000], Loss: 0.6271\n",
            "Epoch [3960/10000], Loss: 0.5712\n",
            "Epoch [3970/10000], Loss: 0.6129\n",
            "Epoch [3980/10000], Loss: 0.5171\n",
            "Epoch [3990/10000], Loss: 0.5348\n",
            "Epoch [4000/10000], Loss: 0.4895\n",
            "Epoch [4010/10000], Loss: 0.5502\n",
            "Epoch [4020/10000], Loss: 0.3842\n",
            "Epoch [4030/10000], Loss: 0.5626\n",
            "Epoch [4040/10000], Loss: 0.5958\n",
            "Epoch [4050/10000], Loss: 0.5035\n",
            "Epoch [4060/10000], Loss: 0.5057\n",
            "Epoch [4070/10000], Loss: 0.4201\n",
            "Epoch [4080/10000], Loss: 0.6027\n",
            "Epoch [4090/10000], Loss: 0.4483\n",
            "Epoch [4100/10000], Loss: 0.5446\n",
            "Epoch [4110/10000], Loss: 0.5504\n",
            "Epoch [4120/10000], Loss: 0.4149\n",
            "Epoch [4130/10000], Loss: 0.4375\n",
            "Epoch [4140/10000], Loss: 0.6214\n",
            "Epoch [4150/10000], Loss: 0.4665\n",
            "Epoch [4160/10000], Loss: 0.4690\n",
            "Epoch [4170/10000], Loss: 0.6023\n",
            "Epoch [4180/10000], Loss: 0.4403\n",
            "Epoch [4190/10000], Loss: 0.5560\n",
            "Epoch [4200/10000], Loss: 0.4663\n",
            "Epoch [4210/10000], Loss: 0.4699\n",
            "Epoch [4220/10000], Loss: 0.4114\n",
            "Epoch [4230/10000], Loss: 0.4674\n",
            "Epoch [4240/10000], Loss: 0.5087\n",
            "Epoch [4250/10000], Loss: 0.7533\n",
            "Epoch [4260/10000], Loss: 0.5575\n",
            "Epoch [4270/10000], Loss: 0.6097\n",
            "Epoch [4280/10000], Loss: 0.6612\n",
            "Epoch [4290/10000], Loss: 0.4650\n",
            "Epoch [4300/10000], Loss: 0.6052\n",
            "Epoch [4310/10000], Loss: 0.5240\n",
            "Epoch [4320/10000], Loss: 0.4974\n",
            "Epoch [4330/10000], Loss: 0.6089\n",
            "Epoch [4340/10000], Loss: 0.5091\n",
            "Epoch [4350/10000], Loss: 0.5556\n",
            "Epoch [4360/10000], Loss: 0.4933\n",
            "Epoch [4370/10000], Loss: 0.6425\n",
            "Epoch [4380/10000], Loss: 0.4601\n",
            "Epoch [4390/10000], Loss: 0.4960\n",
            "Epoch [4400/10000], Loss: 0.5552\n",
            "Epoch [4410/10000], Loss: 0.5981\n",
            "Epoch [4420/10000], Loss: 0.6065\n",
            "Epoch [4430/10000], Loss: 0.6040\n",
            "Epoch [4440/10000], Loss: 0.5284\n",
            "Epoch [4450/10000], Loss: 0.4825\n",
            "Epoch [4460/10000], Loss: 0.4704\n",
            "Epoch [4470/10000], Loss: 0.5006\n",
            "Epoch [4480/10000], Loss: 0.3908\n",
            "Epoch [4490/10000], Loss: 0.4686\n",
            "Epoch [4500/10000], Loss: 0.4851\n",
            "Epoch [4510/10000], Loss: 0.4547\n",
            "Epoch [4520/10000], Loss: 0.3967\n",
            "Epoch [4530/10000], Loss: 0.6105\n",
            "Epoch [4540/10000], Loss: 0.4899\n",
            "Epoch [4550/10000], Loss: 0.4986\n",
            "Epoch [4560/10000], Loss: 0.6100\n",
            "Epoch [4570/10000], Loss: 0.4054\n",
            "Epoch [4580/10000], Loss: 0.3820\n",
            "Epoch [4590/10000], Loss: 0.4336\n",
            "Epoch [4600/10000], Loss: 0.4826\n",
            "Epoch [4610/10000], Loss: 0.5418\n",
            "Epoch [4620/10000], Loss: 0.6448\n",
            "Epoch [4630/10000], Loss: 0.6495\n",
            "Epoch [4640/10000], Loss: 0.4659\n",
            "Epoch [4650/10000], Loss: 0.6434\n",
            "Epoch [4660/10000], Loss: 0.5416\n",
            "Epoch [4670/10000], Loss: 0.5141\n",
            "Epoch [4680/10000], Loss: 0.5886\n",
            "Epoch [4690/10000], Loss: 0.5562\n",
            "Epoch [4700/10000], Loss: 0.6449\n",
            "Epoch [4710/10000], Loss: 0.3776\n",
            "Epoch [4720/10000], Loss: 0.4653\n",
            "Epoch [4730/10000], Loss: 0.5641\n",
            "Epoch [4740/10000], Loss: 0.6018\n",
            "Epoch [4750/10000], Loss: 0.5637\n",
            "Epoch [4760/10000], Loss: 0.5554\n",
            "Epoch [4770/10000], Loss: 0.5893\n",
            "Epoch [4780/10000], Loss: 0.4807\n",
            "Epoch [4790/10000], Loss: 0.4617\n",
            "Epoch [4800/10000], Loss: 0.6131\n",
            "Epoch [4810/10000], Loss: 0.5505\n",
            "Epoch [4820/10000], Loss: 0.5273\n",
            "Epoch [4830/10000], Loss: 0.5496\n",
            "Epoch [4840/10000], Loss: 0.5568\n",
            "Epoch [4850/10000], Loss: 0.5106\n",
            "Epoch [4860/10000], Loss: 0.4420\n",
            "Epoch [4870/10000], Loss: 0.6021\n",
            "Epoch [4880/10000], Loss: 0.5987\n",
            "Epoch [4890/10000], Loss: 0.6030\n",
            "Epoch [4900/10000], Loss: 0.6775\n",
            "Epoch [4910/10000], Loss: 0.5835\n",
            "Epoch [4920/10000], Loss: 0.4306\n",
            "Epoch [4930/10000], Loss: 0.4673\n",
            "Epoch [4940/10000], Loss: 0.5407\n",
            "Epoch [4950/10000], Loss: 0.6405\n",
            "Epoch [4960/10000], Loss: 0.6679\n",
            "Epoch [4970/10000], Loss: 0.5883\n",
            "Epoch [4980/10000], Loss: 0.5335\n",
            "Epoch [4990/10000], Loss: 0.5637\n",
            "Epoch [5000/10000], Loss: 0.4906\n",
            "Epoch [5010/10000], Loss: 0.4334\n",
            "Epoch [5020/10000], Loss: 0.4852\n",
            "Epoch [5030/10000], Loss: 0.4471\n",
            "Epoch [5040/10000], Loss: 0.4119\n",
            "Epoch [5050/10000], Loss: 0.5055\n",
            "Epoch [5060/10000], Loss: 0.5436\n",
            "Epoch [5070/10000], Loss: 0.5882\n",
            "Epoch [5080/10000], Loss: 0.6005\n",
            "Epoch [5090/10000], Loss: 0.7304\n",
            "Epoch [5100/10000], Loss: 0.5553\n",
            "Epoch [5110/10000], Loss: 0.6080\n",
            "Epoch [5120/10000], Loss: 0.5416\n",
            "Epoch [5130/10000], Loss: 0.5491\n",
            "Epoch [5140/10000], Loss: 0.5199\n",
            "Epoch [5150/10000], Loss: 0.6361\n",
            "Epoch [5160/10000], Loss: 0.5481\n",
            "Epoch [5170/10000], Loss: 0.5501\n",
            "Epoch [5180/10000], Loss: 0.4626\n",
            "Epoch [5190/10000], Loss: 0.4967\n",
            "Epoch [5200/10000], Loss: 0.5343\n",
            "Epoch [5210/10000], Loss: 0.3583\n",
            "Epoch [5220/10000], Loss: 0.5116\n",
            "Epoch [5230/10000], Loss: 0.3817\n",
            "Epoch [5240/10000], Loss: 0.4698\n",
            "Epoch [5250/10000], Loss: 0.4412\n",
            "Epoch [5260/10000], Loss: 0.4245\n",
            "Epoch [5270/10000], Loss: 0.5641\n",
            "Epoch [5280/10000], Loss: 0.6452\n",
            "Epoch [5290/10000], Loss: 0.3654\n",
            "Epoch [5300/10000], Loss: 0.4337\n",
            "Epoch [5310/10000], Loss: 0.4894\n",
            "Epoch [5320/10000], Loss: 0.4394\n",
            "Epoch [5330/10000], Loss: 0.5213\n",
            "Epoch [5340/10000], Loss: 0.6095\n",
            "Epoch [5350/10000], Loss: 0.4600\n",
            "Epoch [5360/10000], Loss: 0.4386\n",
            "Epoch [5370/10000], Loss: 0.6012\n",
            "Epoch [5380/10000], Loss: 0.5045\n",
            "Epoch [5390/10000], Loss: 0.4442\n",
            "Epoch [5400/10000], Loss: 0.4655\n",
            "Epoch [5410/10000], Loss: 0.4973\n",
            "Epoch [5420/10000], Loss: 0.4586\n",
            "Epoch [5430/10000], Loss: 0.4663\n",
            "Epoch [5440/10000], Loss: 0.6533\n",
            "Epoch [5450/10000], Loss: 0.5220\n",
            "Epoch [5460/10000], Loss: 0.4420\n",
            "Epoch [5470/10000], Loss: 0.6187\n",
            "Epoch [5480/10000], Loss: 0.4594\n",
            "Epoch [5490/10000], Loss: 0.6155\n",
            "Epoch [5500/10000], Loss: 0.4910\n",
            "Epoch [5510/10000], Loss: 0.5039\n",
            "Epoch [5520/10000], Loss: 0.4443\n",
            "Epoch [5530/10000], Loss: 0.5780\n",
            "Epoch [5540/10000], Loss: 0.4884\n",
            "Epoch [5550/10000], Loss: 0.6116\n",
            "Epoch [5560/10000], Loss: 0.5411\n",
            "Epoch [5570/10000], Loss: 0.5244\n",
            "Epoch [5580/10000], Loss: 0.5423\n",
            "Epoch [5590/10000], Loss: 0.6103\n",
            "Epoch [5600/10000], Loss: 0.4321\n",
            "Epoch [5610/10000], Loss: 0.4891\n",
            "Epoch [5620/10000], Loss: 0.5432\n",
            "Epoch [5630/10000], Loss: 0.5175\n",
            "Epoch [5640/10000], Loss: 0.6343\n",
            "Epoch [5650/10000], Loss: 0.5812\n",
            "Epoch [5660/10000], Loss: 0.5413\n",
            "Epoch [5670/10000], Loss: 0.5246\n",
            "Epoch [5680/10000], Loss: 0.6049\n",
            "Epoch [5690/10000], Loss: 0.6742\n",
            "Epoch [5700/10000], Loss: 0.4372\n",
            "Epoch [5710/10000], Loss: 0.6428\n",
            "Epoch [5720/10000], Loss: 0.5780\n",
            "Epoch [5730/10000], Loss: 0.5984\n",
            "Epoch [5740/10000], Loss: 0.3965\n",
            "Epoch [5750/10000], Loss: 0.4353\n",
            "Epoch [5760/10000], Loss: 0.5909\n",
            "Epoch [5770/10000], Loss: 0.6201\n",
            "Epoch [5780/10000], Loss: 0.5118\n",
            "Epoch [5790/10000], Loss: 0.5149\n",
            "Epoch [5800/10000], Loss: 0.5917\n",
            "Epoch [5810/10000], Loss: 0.7424\n",
            "Epoch [5820/10000], Loss: 0.5820\n",
            "Epoch [5830/10000], Loss: 0.4366\n",
            "Epoch [5840/10000], Loss: 0.4894\n",
            "Epoch [5850/10000], Loss: 0.5877\n",
            "Epoch [5860/10000], Loss: 0.6022\n",
            "Epoch [5870/10000], Loss: 0.7213\n",
            "Epoch [5880/10000], Loss: 0.5727\n",
            "Epoch [5890/10000], Loss: 0.6103\n",
            "Epoch [5900/10000], Loss: 0.5089\n",
            "Epoch [5910/10000], Loss: 0.6122\n",
            "Epoch [5920/10000], Loss: 0.5218\n",
            "Epoch [5930/10000], Loss: 0.3890\n",
            "Epoch [5940/10000], Loss: 0.5792\n",
            "Epoch [5950/10000], Loss: 0.5220\n",
            "Epoch [5960/10000], Loss: 0.5182\n",
            "Epoch [5970/10000], Loss: 0.5015\n",
            "Epoch [5980/10000], Loss: 0.5027\n",
            "Epoch [5990/10000], Loss: 0.3599\n",
            "Epoch [6000/10000], Loss: 0.6213\n",
            "Epoch [6010/10000], Loss: 0.4881\n",
            "Epoch [6020/10000], Loss: 0.5406\n",
            "Epoch [6030/10000], Loss: 0.5248\n",
            "Epoch [6040/10000], Loss: 0.4252\n",
            "Epoch [6050/10000], Loss: 0.4395\n",
            "Epoch [6060/10000], Loss: 0.5560\n",
            "Epoch [6070/10000], Loss: 0.4083\n",
            "Epoch [6080/10000], Loss: 0.5318\n",
            "Epoch [6090/10000], Loss: 0.6169\n",
            "Epoch [6100/10000], Loss: 0.4608\n",
            "Epoch [6110/10000], Loss: 0.4817\n",
            "Epoch [6120/10000], Loss: 0.3847\n",
            "Epoch [6130/10000], Loss: 0.4683\n",
            "Epoch [6140/10000], Loss: 0.4792\n",
            "Epoch [6150/10000], Loss: 0.5739\n",
            "Epoch [6160/10000], Loss: 0.4862\n",
            "Epoch [6170/10000], Loss: 0.4898\n",
            "Epoch [6180/10000], Loss: 0.5513\n",
            "Epoch [6190/10000], Loss: 0.5412\n",
            "Epoch [6200/10000], Loss: 0.4899\n",
            "Epoch [6210/10000], Loss: 0.5634\n",
            "Epoch [6220/10000], Loss: 0.4783\n",
            "Epoch [6230/10000], Loss: 0.5424\n",
            "Epoch [6240/10000], Loss: 0.4142\n",
            "Epoch [6250/10000], Loss: 0.5632\n",
            "Epoch [6260/10000], Loss: 0.4801\n",
            "Epoch [6270/10000], Loss: 0.5598\n",
            "Epoch [6280/10000], Loss: 0.4396\n",
            "Epoch [6290/10000], Loss: 0.5133\n",
            "Epoch [6300/10000], Loss: 0.5101\n",
            "Epoch [6310/10000], Loss: 0.7344\n",
            "Epoch [6320/10000], Loss: 0.6183\n",
            "Epoch [6330/10000], Loss: 0.4590\n",
            "Epoch [6340/10000], Loss: 0.4954\n",
            "Epoch [6350/10000], Loss: 0.5762\n",
            "Epoch [6360/10000], Loss: 0.4632\n",
            "Epoch [6370/10000], Loss: 0.5415\n",
            "Epoch [6380/10000], Loss: 0.4872\n",
            "Epoch [6390/10000], Loss: 0.5369\n",
            "Epoch [6400/10000], Loss: 0.4370\n",
            "Epoch [6410/10000], Loss: 0.3817\n",
            "Epoch [6420/10000], Loss: 0.5629\n",
            "Epoch [6430/10000], Loss: 0.4808\n",
            "Epoch [6440/10000], Loss: 0.5274\n",
            "Epoch [6450/10000], Loss: 0.4991\n",
            "Epoch [6460/10000], Loss: 0.4639\n",
            "Epoch [6470/10000], Loss: 0.5555\n",
            "Epoch [6480/10000], Loss: 0.4248\n",
            "Epoch [6490/10000], Loss: 0.4436\n",
            "Epoch [6500/10000], Loss: 0.5551\n",
            "Epoch [6510/10000], Loss: 0.6726\n",
            "Epoch [6520/10000], Loss: 0.5628\n",
            "Epoch [6530/10000], Loss: 0.5423\n",
            "Epoch [6540/10000], Loss: 0.4837\n",
            "Epoch [6550/10000], Loss: 0.5635\n",
            "Epoch [6560/10000], Loss: 0.5419\n",
            "Epoch [6570/10000], Loss: 0.5807\n",
            "Epoch [6580/10000], Loss: 0.4451\n",
            "Epoch [6590/10000], Loss: 0.6013\n",
            "Epoch [6600/10000], Loss: 0.5408\n",
            "Epoch [6610/10000], Loss: 0.4568\n",
            "Epoch [6620/10000], Loss: 0.6200\n",
            "Epoch [6630/10000], Loss: 0.7393\n",
            "Epoch [6640/10000], Loss: 0.5554\n",
            "Epoch [6650/10000], Loss: 0.5877\n",
            "Epoch [6660/10000], Loss: 0.4627\n",
            "Epoch [6670/10000], Loss: 0.5421\n",
            "Epoch [6680/10000], Loss: 0.5917\n",
            "Epoch [6690/10000], Loss: 0.5351\n",
            "Epoch [6700/10000], Loss: 0.3780\n",
            "Epoch [6710/10000], Loss: 0.5340\n",
            "Epoch [6720/10000], Loss: 0.5368\n",
            "Epoch [6730/10000], Loss: 0.5778\n",
            "Epoch [6740/10000], Loss: 0.5865\n",
            "Epoch [6750/10000], Loss: 0.4446\n",
            "Epoch [6760/10000], Loss: 0.4786\n",
            "Epoch [6770/10000], Loss: 0.4544\n",
            "Epoch [6780/10000], Loss: 0.5497\n",
            "Epoch [6790/10000], Loss: 0.5108\n",
            "Epoch [6800/10000], Loss: 0.5148\n",
            "Epoch [6810/10000], Loss: 0.5965\n",
            "Epoch [6820/10000], Loss: 0.5029\n",
            "Epoch [6830/10000], Loss: 0.5038\n",
            "Epoch [6840/10000], Loss: 0.4818\n",
            "Epoch [6850/10000], Loss: 0.5490\n",
            "Epoch [6860/10000], Loss: 0.4859\n",
            "Epoch [6870/10000], Loss: 0.4153\n",
            "Epoch [6880/10000], Loss: 0.5581\n",
            "Epoch [6890/10000], Loss: 0.4371\n",
            "Epoch [6900/10000], Loss: 0.4381\n",
            "Epoch [6910/10000], Loss: 0.4432\n",
            "Epoch [6920/10000], Loss: 0.5892\n",
            "Epoch [6930/10000], Loss: 0.5601\n",
            "Epoch [6940/10000], Loss: 0.4618\n",
            "Epoch [6950/10000], Loss: 0.6815\n",
            "Epoch [6960/10000], Loss: 0.3519\n",
            "Epoch [6970/10000], Loss: 0.5632\n",
            "Epoch [6980/10000], Loss: 0.4842\n",
            "Epoch [6990/10000], Loss: 0.5444\n",
            "Epoch [7000/10000], Loss: 0.6182\n",
            "Epoch [7010/10000], Loss: 0.5030\n",
            "Epoch [7020/10000], Loss: 0.5419\n",
            "Epoch [7030/10000], Loss: 0.4533\n",
            "Epoch [7040/10000], Loss: 0.6102\n",
            "Epoch [7050/10000], Loss: 0.6749\n",
            "Epoch [7060/10000], Loss: 0.5873\n",
            "Epoch [7070/10000], Loss: 0.6021\n",
            "Epoch [7080/10000], Loss: 0.5713\n",
            "Epoch [7090/10000], Loss: 0.4038\n",
            "Epoch [7100/10000], Loss: 0.5685\n",
            "Epoch [7110/10000], Loss: 0.4126\n",
            "Epoch [7120/10000], Loss: 0.5994\n",
            "Epoch [7130/10000], Loss: 0.6563\n",
            "Epoch [7140/10000], Loss: 0.6026\n",
            "Epoch [7150/10000], Loss: 0.4395\n",
            "Epoch [7160/10000], Loss: 0.5165\n",
            "Epoch [7170/10000], Loss: 0.6597\n",
            "Epoch [7180/10000], Loss: 0.6106\n",
            "Epoch [7190/10000], Loss: 0.4861\n",
            "Epoch [7200/10000], Loss: 0.4432\n",
            "Epoch [7210/10000], Loss: 0.4380\n",
            "Epoch [7220/10000], Loss: 0.4712\n",
            "Epoch [7230/10000], Loss: 0.4418\n",
            "Epoch [7240/10000], Loss: 0.4546\n",
            "Epoch [7250/10000], Loss: 0.5409\n",
            "Epoch [7260/10000], Loss: 0.5607\n",
            "Epoch [7270/10000], Loss: 0.4841\n",
            "Epoch [7280/10000], Loss: 0.4909\n",
            "Epoch [7290/10000], Loss: 0.5338\n",
            "Epoch [7300/10000], Loss: 0.5338\n",
            "Epoch [7310/10000], Loss: 0.5280\n",
            "Epoch [7320/10000], Loss: 0.6642\n",
            "Epoch [7330/10000], Loss: 0.6784\n",
            "Epoch [7340/10000], Loss: 0.5038\n",
            "Epoch [7350/10000], Loss: 0.5361\n",
            "Epoch [7360/10000], Loss: 0.5823\n",
            "Epoch [7370/10000], Loss: 0.6226\n",
            "Epoch [7380/10000], Loss: 0.4449\n",
            "Epoch [7390/10000], Loss: 0.5495\n",
            "Epoch [7400/10000], Loss: 0.6606\n",
            "Epoch [7410/10000], Loss: 0.4646\n",
            "Epoch [7420/10000], Loss: 0.7533\n",
            "Epoch [7430/10000], Loss: 0.6005\n",
            "Epoch [7440/10000], Loss: 0.4652\n",
            "Epoch [7450/10000], Loss: 0.5885\n",
            "Epoch [7460/10000], Loss: 0.6194\n",
            "Epoch [7470/10000], Loss: 0.4279\n",
            "Epoch [7480/10000], Loss: 0.5639\n",
            "Epoch [7490/10000], Loss: 0.5561\n",
            "Epoch [7500/10000], Loss: 0.5557\n",
            "Epoch [7510/10000], Loss: 0.4696\n",
            "Epoch [7520/10000], Loss: 0.4660\n",
            "Epoch [7530/10000], Loss: 0.6069\n",
            "Epoch [7540/10000], Loss: 0.7341\n",
            "Epoch [7550/10000], Loss: 0.4853\n",
            "Epoch [7560/10000], Loss: 0.5310\n",
            "Epoch [7570/10000], Loss: 0.4065\n",
            "Epoch [7580/10000], Loss: 0.4336\n",
            "Epoch [7590/10000], Loss: 0.5225\n",
            "Epoch [7600/10000], Loss: 0.5351\n",
            "Epoch [7610/10000], Loss: 0.6943\n",
            "Epoch [7620/10000], Loss: 0.4531\n",
            "Epoch [7630/10000], Loss: 0.4368\n",
            "Epoch [7640/10000], Loss: 0.5256\n",
            "Epoch [7650/10000], Loss: 0.5422\n",
            "Epoch [7660/10000], Loss: 0.5001\n",
            "Epoch [7670/10000], Loss: 0.5341\n",
            "Epoch [7680/10000], Loss: 0.6291\n",
            "Epoch [7690/10000], Loss: 0.4542\n",
            "Epoch [7700/10000], Loss: 0.5822\n",
            "Epoch [7710/10000], Loss: 0.4843\n",
            "Epoch [7720/10000], Loss: 0.5735\n",
            "Epoch [7730/10000], Loss: 0.5248\n",
            "Epoch [7740/10000], Loss: 0.5002\n",
            "Epoch [7750/10000], Loss: 0.6021\n",
            "Epoch [7760/10000], Loss: 0.5178\n",
            "Epoch [7770/10000], Loss: 0.4450\n",
            "Epoch [7780/10000], Loss: 0.7325\n",
            "Epoch [7790/10000], Loss: 0.5561\n",
            "Epoch [7800/10000], Loss: 0.4881\n",
            "Epoch [7810/10000], Loss: 0.5196\n",
            "Epoch [7820/10000], Loss: 0.5562\n",
            "Epoch [7830/10000], Loss: 0.4664\n",
            "Epoch [7840/10000], Loss: 0.5368\n",
            "Epoch [7850/10000], Loss: 0.4892\n",
            "Epoch [7860/10000], Loss: 0.3846\n",
            "Epoch [7870/10000], Loss: 0.4373\n",
            "Epoch [7880/10000], Loss: 0.3830\n",
            "Epoch [7890/10000], Loss: 0.3465\n",
            "Epoch [7900/10000], Loss: 0.6028\n",
            "Epoch [7910/10000], Loss: 0.5354\n",
            "Epoch [7920/10000], Loss: 0.4899\n",
            "Epoch [7930/10000], Loss: 0.5525\n",
            "Epoch [7940/10000], Loss: 0.5836\n",
            "Epoch [7950/10000], Loss: 0.5359\n",
            "Epoch [7960/10000], Loss: 0.5744\n",
            "Epoch [7970/10000], Loss: 0.5893\n",
            "Epoch [7980/10000], Loss: 0.4118\n",
            "Epoch [7990/10000], Loss: 0.6320\n",
            "Epoch [8000/10000], Loss: 0.5437\n",
            "Epoch [8010/10000], Loss: 0.4768\n",
            "Epoch [8020/10000], Loss: 0.4623\n",
            "Epoch [8030/10000], Loss: 0.5784\n",
            "Epoch [8040/10000], Loss: 0.5044\n",
            "Epoch [8050/10000], Loss: 0.5339\n",
            "Epoch [8060/10000], Loss: 0.5302\n",
            "Epoch [8070/10000], Loss: 0.4359\n",
            "Epoch [8080/10000], Loss: 0.6109\n",
            "Epoch [8090/10000], Loss: 0.4636\n",
            "Epoch [8100/10000], Loss: 0.4580\n",
            "Epoch [8110/10000], Loss: 0.4381\n",
            "Epoch [8120/10000], Loss: 0.5185\n",
            "Epoch [8130/10000], Loss: 0.5147\n",
            "Epoch [8140/10000], Loss: 0.5421\n",
            "Epoch [8150/10000], Loss: 0.5505\n",
            "Epoch [8160/10000], Loss: 0.5654\n",
            "Epoch [8170/10000], Loss: 0.4422\n",
            "Epoch [8180/10000], Loss: 0.5140\n",
            "Epoch [8190/10000], Loss: 0.4960\n",
            "Epoch [8200/10000], Loss: 0.5504\n",
            "Epoch [8210/10000], Loss: 0.5170\n",
            "Epoch [8220/10000], Loss: 0.5107\n",
            "Epoch [8230/10000], Loss: 0.5549\n",
            "Epoch [8240/10000], Loss: 0.4005\n",
            "Epoch [8250/10000], Loss: 0.5848\n",
            "Epoch [8260/10000], Loss: 0.6004\n",
            "Epoch [8270/10000], Loss: 0.5418\n",
            "Epoch [8280/10000], Loss: 0.5917\n",
            "Epoch [8290/10000], Loss: 0.5513\n",
            "Epoch [8300/10000], Loss: 0.5600\n",
            "Epoch [8310/10000], Loss: 0.4368\n",
            "Epoch [8320/10000], Loss: 0.4502\n",
            "Epoch [8330/10000], Loss: 0.5212\n",
            "Epoch [8340/10000], Loss: 0.6875\n",
            "Epoch [8350/10000], Loss: 0.4723\n",
            "Epoch [8360/10000], Loss: 0.5261\n",
            "Epoch [8370/10000], Loss: 0.4109\n",
            "Epoch [8380/10000], Loss: 0.5345\n",
            "Epoch [8390/10000], Loss: 0.8333\n",
            "Epoch [8400/10000], Loss: 0.6755\n",
            "Epoch [8410/10000], Loss: 0.5631\n",
            "Epoch [8420/10000], Loss: 0.4879\n",
            "Epoch [8430/10000], Loss: 0.5512\n",
            "Epoch [8440/10000], Loss: 0.4832\n",
            "Epoch [8450/10000], Loss: 0.5409\n",
            "Epoch [8460/10000], Loss: 0.4402\n",
            "Epoch [8470/10000], Loss: 0.5630\n",
            "Epoch [8480/10000], Loss: 0.5430\n",
            "Epoch [8490/10000], Loss: 0.4858\n",
            "Epoch [8500/10000], Loss: 0.5836\n",
            "Epoch [8510/10000], Loss: 0.4080\n",
            "Epoch [8520/10000], Loss: 0.4823\n",
            "Epoch [8530/10000], Loss: 0.5506\n",
            "Epoch [8540/10000], Loss: 0.6038\n",
            "Epoch [8550/10000], Loss: 0.4747\n",
            "Epoch [8560/10000], Loss: 0.6399\n",
            "Epoch [8570/10000], Loss: 0.5425\n",
            "Epoch [8580/10000], Loss: 0.5345\n",
            "Epoch [8590/10000], Loss: 0.5631\n",
            "Epoch [8600/10000], Loss: 0.4390\n",
            "Epoch [8610/10000], Loss: 0.6738\n",
            "Epoch [8620/10000], Loss: 0.4734\n",
            "Epoch [8630/10000], Loss: 0.5415\n",
            "Epoch [8640/10000], Loss: 0.5235\n",
            "Epoch [8650/10000], Loss: 0.4859\n",
            "Epoch [8660/10000], Loss: 0.5955\n",
            "Epoch [8670/10000], Loss: 0.4452\n",
            "Epoch [8680/10000], Loss: 0.3556\n",
            "Epoch [8690/10000], Loss: 0.4386\n",
            "Epoch [8700/10000], Loss: 0.6069\n",
            "Epoch [8710/10000], Loss: 0.4928\n",
            "Epoch [8720/10000], Loss: 0.5283\n",
            "Epoch [8730/10000], Loss: 0.5520\n",
            "Epoch [8740/10000], Loss: 0.4671\n",
            "Epoch [8750/10000], Loss: 0.5501\n",
            "Epoch [8760/10000], Loss: 0.6670\n",
            "Epoch [8770/10000], Loss: 0.5558\n",
            "Epoch [8780/10000], Loss: 0.6201\n",
            "Epoch [8790/10000], Loss: 0.4069\n",
            "Epoch [8800/10000], Loss: 0.4831\n",
            "Epoch [8810/10000], Loss: 0.5357\n",
            "Epoch [8820/10000], Loss: 0.4926\n",
            "Epoch [8830/10000], Loss: 0.4659\n",
            "Epoch [8840/10000], Loss: 0.5395\n",
            "Epoch [8850/10000], Loss: 0.5129\n",
            "Epoch [8860/10000], Loss: 0.5015\n",
            "Epoch [8870/10000], Loss: 0.6348\n",
            "Epoch [8880/10000], Loss: 0.5415\n",
            "Epoch [8890/10000], Loss: 0.5259\n",
            "Epoch [8900/10000], Loss: 0.5129\n",
            "Epoch [8910/10000], Loss: 0.5250\n",
            "Epoch [8920/10000], Loss: 0.4819\n",
            "Epoch [8930/10000], Loss: 0.6693\n",
            "Epoch [8940/10000], Loss: 0.5249\n",
            "Epoch [8950/10000], Loss: 0.5824\n",
            "Epoch [8960/10000], Loss: 0.5353\n",
            "Epoch [8970/10000], Loss: 0.6329\n",
            "Epoch [8980/10000], Loss: 0.4426\n",
            "Epoch [8990/10000], Loss: 0.5979\n",
            "Epoch [9000/10000], Loss: 0.6881\n",
            "Epoch [9010/10000], Loss: 0.4857\n",
            "Epoch [9020/10000], Loss: 0.4649\n",
            "Epoch [9030/10000], Loss: 0.6485\n",
            "Epoch [9040/10000], Loss: 0.4440\n",
            "Epoch [9050/10000], Loss: 0.4540\n",
            "Epoch [9060/10000], Loss: 0.3939\n",
            "Epoch [9070/10000], Loss: 0.5236\n",
            "Epoch [9080/10000], Loss: 0.6475\n",
            "Epoch [9090/10000], Loss: 0.6064\n",
            "Epoch [9100/10000], Loss: 0.4935\n",
            "Epoch [9110/10000], Loss: 0.5098\n",
            "Epoch [9120/10000], Loss: 0.4887\n",
            "Epoch [9130/10000], Loss: 0.4310\n",
            "Epoch [9140/10000], Loss: 0.3915\n",
            "Epoch [9150/10000], Loss: 0.4838\n",
            "Epoch [9160/10000], Loss: 0.6638\n",
            "Epoch [9170/10000], Loss: 0.6206\n",
            "Epoch [9180/10000], Loss: 0.6174\n",
            "Epoch [9190/10000], Loss: 0.5354\n",
            "Epoch [9200/10000], Loss: 0.3923\n",
            "Epoch [9210/10000], Loss: 0.5017\n",
            "Epoch [9220/10000], Loss: 0.3830\n",
            "Epoch [9230/10000], Loss: 0.6473\n",
            "Epoch [9240/10000], Loss: 0.5633\n",
            "Epoch [9250/10000], Loss: 0.4411\n",
            "Epoch [9260/10000], Loss: 0.6044\n",
            "Epoch [9270/10000], Loss: 0.5596\n",
            "Epoch [9280/10000], Loss: 0.5554\n",
            "Epoch [9290/10000], Loss: 0.6221\n",
            "Epoch [9300/10000], Loss: 0.5094\n",
            "Epoch [9310/10000], Loss: 0.5635\n",
            "Epoch [9320/10000], Loss: 0.7521\n",
            "Epoch [9330/10000], Loss: 0.6117\n",
            "Epoch [9340/10000], Loss: 0.5501\n",
            "Epoch [9350/10000], Loss: 0.4125\n",
            "Epoch [9360/10000], Loss: 0.6646\n",
            "Epoch [9370/10000], Loss: 0.5592\n",
            "Epoch [9380/10000], Loss: 0.5249\n",
            "Epoch [9390/10000], Loss: 0.4663\n",
            "Epoch [9400/10000], Loss: 0.5412\n",
            "Epoch [9410/10000], Loss: 0.6926\n",
            "Epoch [9420/10000], Loss: 0.4352\n",
            "Epoch [9430/10000], Loss: 0.5410\n",
            "Epoch [9440/10000], Loss: 0.5895\n",
            "Epoch [9450/10000], Loss: 0.5254\n",
            "Epoch [9460/10000], Loss: 0.5112\n",
            "Epoch [9470/10000], Loss: 0.3847\n",
            "Epoch [9480/10000], Loss: 0.4961\n",
            "Epoch [9490/10000], Loss: 0.4740\n",
            "Epoch [9500/10000], Loss: 0.5339\n",
            "Epoch [9510/10000], Loss: 0.5409\n",
            "Epoch [9520/10000], Loss: 0.4852\n",
            "Epoch [9530/10000], Loss: 0.6646\n",
            "Epoch [9540/10000], Loss: 0.5056\n",
            "Epoch [9550/10000], Loss: 0.4399\n",
            "Epoch [9560/10000], Loss: 0.4867\n",
            "Epoch [9570/10000], Loss: 0.5630\n",
            "Epoch [9580/10000], Loss: 0.5251\n",
            "Epoch [9590/10000], Loss: 0.4996\n",
            "Epoch [9600/10000], Loss: 0.4182\n",
            "Epoch [9610/10000], Loss: 0.5642\n",
            "Epoch [9620/10000], Loss: 0.4830\n",
            "Epoch [9630/10000], Loss: 0.5416\n",
            "Epoch [9640/10000], Loss: 0.5009\n",
            "Epoch [9650/10000], Loss: 0.5615\n",
            "Epoch [9660/10000], Loss: 0.5472\n",
            "Epoch [9670/10000], Loss: 0.4821\n",
            "Epoch [9680/10000], Loss: 0.5110\n",
            "Epoch [9690/10000], Loss: 0.5949\n",
            "Epoch [9700/10000], Loss: 0.4523\n",
            "Epoch [9710/10000], Loss: 0.4071\n",
            "Epoch [9720/10000], Loss: 0.4890\n",
            "Epoch [9730/10000], Loss: 0.6461\n",
            "Epoch [9740/10000], Loss: 0.5552\n",
            "Epoch [9750/10000], Loss: 0.3968\n",
            "Epoch [9760/10000], Loss: 0.6190\n",
            "Epoch [9770/10000], Loss: 0.4866\n",
            "Epoch [9780/10000], Loss: 0.4660\n",
            "Epoch [9790/10000], Loss: 0.4865\n",
            "Epoch [9800/10000], Loss: 0.3857\n",
            "Epoch [9810/10000], Loss: 0.4645\n",
            "Epoch [9820/10000], Loss: 0.5416\n",
            "Epoch [9830/10000], Loss: 0.4241\n",
            "Epoch [9840/10000], Loss: 0.4819\n",
            "Epoch [9850/10000], Loss: 0.3845\n",
            "Epoch [9860/10000], Loss: 0.4801\n",
            "Epoch [9870/10000], Loss: 0.5231\n",
            "Epoch [9880/10000], Loss: 0.5258\n",
            "Epoch [9890/10000], Loss: 0.5103\n",
            "Epoch [9900/10000], Loss: 0.5514\n",
            "Epoch [9910/10000], Loss: 0.3712\n",
            "Epoch [9920/10000], Loss: 0.4869\n",
            "Epoch [9930/10000], Loss: 0.6185\n",
            "Epoch [9940/10000], Loss: 0.6139\n",
            "Epoch [9950/10000], Loss: 0.6632\n",
            "Epoch [9960/10000], Loss: 0.6335\n",
            "Epoch [9970/10000], Loss: 0.5100\n",
            "Epoch [9980/10000], Loss: 0.4538\n",
            "Epoch [9990/10000], Loss: 0.5244\n",
            "Epoch [10000/10000], Loss: 0.6244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSBRc2w-A6tw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "30eec89a-3cee-4c8d-c5d0-bc82a5d6cedf"
      },
      "source": [
        "for x, y in train_gen:\n",
        "  print(model(x), y)\n",
        "  print(loss_fn(model(x), y))\n",
        "  break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[100.3147, 118.6247],\n",
            "        [120.3383, 133.4784],\n",
            "        [100.3147, 118.6247],\n",
            "        [ 57.3306,  70.2976],\n",
            "        [100.3147, 118.6247],\n",
            "        [120.3383, 133.4784],\n",
            "        [ 81.4057, 100.4553],\n",
            "        [ 21.5492,  37.1658]], grad_fn=<AddmmBackward>) tensor([[103., 119.],\n",
            "        [119., 133.],\n",
            "        [103., 119.],\n",
            "        [ 56.,  70.],\n",
            "        [103., 119.],\n",
            "        [119., 133.],\n",
            "        [ 81., 101.],\n",
            "        [ 22.,  37.]])\n",
            "tensor(1.7904, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULrNTh4qBIMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b1b89fe2-8cef-444b-cc0f-b06abadb540a"
      },
      "source": [
        "for x, y in train_gen:\n",
        "  print(model(x), y)\n",
        "  print(loss_fn(model(x), y))\n",
        "  break"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[118.7318, 132.9535],\n",
            "        [101.9682, 119.1220],\n",
            "        [ 57.2518,  70.2584],\n",
            "        [118.7318, 132.9535],\n",
            "        [101.9682, 119.1220],\n",
            "        [ 21.1025,  37.0136],\n",
            "        [118.7318, 132.9535],\n",
            "        [ 82.2107, 100.6880]], grad_fn=<AddmmBackward>) tensor([[119., 133.],\n",
            "        [103., 119.],\n",
            "        [ 56.,  70.],\n",
            "        [119., 133.],\n",
            "        [103., 119.],\n",
            "        [ 22.,  37.],\n",
            "        [119., 133.],\n",
            "        [ 81., 101.]])\n",
            "tensor(0.3990, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1PI9uOiBpho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}